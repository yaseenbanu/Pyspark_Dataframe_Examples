{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "Using PySpark transformations, merge data from two input files, 'employee.csv' and 'employee_personal.csv', to create an 'employee_fact' DataFrame. This DataFrame should include the following columns:\n",
    "\n",
    "- `employee_id`\n",
    "- `employee_full_name`\n",
    "- `department`\n",
    "- `salary`\n",
    "- `Salary_Diff_to_reach_highest_sal`\n",
    "- `DOB`\n",
    "- `state`\n",
    "- `country`\n",
    "- `age`\n",
    "\n",
    "Ensure that the 'employee_fact' DataFrame is structured to contain comprehensive information about each employee, including personal details such as full name, date of birth, state, and country, alongside professional details like department and salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('challenges').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+\n",
      "|employee_id|  department| salary|\n",
      "+-----------+------------+-------+\n",
      "|          1|       Sales|50000.0|\n",
      "|          2|   Marketing|60000.0|\n",
      "|          3| Engineering|70000.0|\n",
      "|          4|          HR|55000.0|\n",
      "|          5|     Finance|65000.0|\n",
      "+-----------+------------+-------+\n",
      "\n",
      "+-----------+----------+---------+-------------------+-----------+-------+\n",
      "|employee_id|first_name|last_name|                DOB|      state|country|\n",
      "+-----------+----------+---------+-------------------+-----------+-------+\n",
      "|          1|      John|      Doe|1980-05-15 00:00:00| California|    USA|\n",
      "|          2|      Jane|    Smith|1985-09-20 00:00:00|   New York|    USA|\n",
      "|          3|   Michael|  Johnson|1978-03-10 00:00:00|      Texas|    USA|\n",
      "|          4|     Emily|    Brown|1990-11-25 00:00:00|    Florida|    USA|\n",
      "|          5|     David|    Jones|1982-07-08 00:00:00|   Illinois|    USA|\n",
      "+-----------+----------+---------+-------------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_location = 'data/employee_data'\n",
    "\n",
    "df_input_emp = spark.read.format('csv').options(inferSchema=True,header=True).load(fr\"{employees_location}/employee.csv\")\n",
    "\n",
    "df_input_emp.show()\n",
    "\n",
    "df_input_emp_per = spark.read.format('csv').options(inferSchema=True,header=True).load(fr\"{employees_location}/employee_personal.csv\")\n",
    "\n",
    "df_input_emp_per.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+----------+---------+-------------------+-----------+-------+\n",
      "|employee_id|  department| salary|first_name|last_name|                DOB|      state|country|\n",
      "+-----------+------------+-------+----------+---------+-------------------+-----------+-------+\n",
      "|          1|       Sales|50000.0|      John|      Doe|1980-05-15 00:00:00| California|    USA|\n",
      "|          2|   Marketing|60000.0|      Jane|    Smith|1985-09-20 00:00:00|   New York|    USA|\n",
      "|          3| Engineering|70000.0|   Michael|  Johnson|1978-03-10 00:00:00|      Texas|    USA|\n",
      "|          4|          HR|55000.0|     Emily|    Brown|1990-11-25 00:00:00|    Florida|    USA|\n",
      "|          5|     Finance|65000.0|     David|    Jones|1982-07-08 00:00:00|   Illinois|    USA|\n",
      "+-----------+------------+-------+----------+---------+-------------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_input_emp.join(df_input_emp_per, on='employee_id', how = 'inner')\n",
    "\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- DOB: timestamp (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------+-------+--------------------------------+-------------------+-----------+-------+-----+\n",
      "|employee_id|        full_name|  department| salary|Salary_Diff_to_reach_highest_sal|                dob|      state|country|  Age|\n",
      "+-----------+-----------------+------------+-------+--------------------------------+-------------------+-----------+-------+-----+\n",
      "|          1|        John  Doe|       Sales|50000.0|                         20000.0|1980-05-15 00:00:00| California|    USA|44.61|\n",
      "|          2|      Jane  Smith|   Marketing|60000.0|                         10000.0|1985-09-20 00:00:00|   New York|    USA|39.18|\n",
      "|          3| Michael  Johnson| Engineering|70000.0|                             0.0|1978-03-10 00:00:00|      Texas|    USA|46.82|\n",
      "|          4|     Emily  Brown|          HR|55000.0|                         15000.0|1990-11-25 00:00:00|    Florida|    USA|33.93|\n",
      "|          5|     David  Jones|     Finance|65000.0|                          5000.0|1982-07-08 00:00:00|   Illinois|    USA|42.43|\n",
      "+-----------+-----------------+------------+-------+--------------------------------+-------------------+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "highest_salary = df_join.select('salary').agg(max('salary')).rdd.map(lambda a: a[0]).collect()[0]\n",
    "\n",
    "df = df_join.select(\n",
    "    'employee_id', \n",
    "    concat(col('first_name'), lit(' '), col('last_name')).alias('full_name'),\n",
    "    'department',\n",
    "    'salary',\n",
    "    (lit(highest_salary)-col('salary')).alias('Salary_Diff_to_reach_highest_sal'),\n",
    "    'dob',\n",
    "    'state',\n",
    "    'country',\n",
    "    round(datediff(current_timestamp(),col('dob'))/360,2).alias('Age')\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70000.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
